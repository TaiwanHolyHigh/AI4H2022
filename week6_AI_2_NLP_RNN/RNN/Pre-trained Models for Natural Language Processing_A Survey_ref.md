
#
- [Pre-trained Models for Natural Language Processing:A Survey]()
- [A survey of model compression and acceleration for deep neural networks]()

- Transformer-XL: Attentive language models beyond a fixed-length context


[1]Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network for modelling sentences. In
ACL, 2014.

[2] Yoon Kim. Convolutional neural networks for sentence classification. In EMNLP, pages 1746–1751, 2014.

[3] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence
learning. In ICML, pages 1243–1252, 2017.

[4] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In NeurIPS, pages
3104–3112, 2014.

[5] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Recurrent neural network for text classification with multi-task learning.
In IJCAI, 2016.

[6] Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts.
Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, pages 1631–1642. ACL, 2013.

[7] Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representations from tree-structured long
short-term memory networks. In ACL, pages 1556–1566, 2015.

[8] Diego Marcheggiani, Joost Bastings, and Ivan Titov. Exploiting semantics in neural machine translation with graph convolutional networks. In NAACL-HLT, pages 486–492, 2018.

[9] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and
translate. In ICLR, 2014.

[10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS, 2017.

[11] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representations of words
and phrases and their compositionality. In NeurIPS, 2013.

[12] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. GloVe: Global vectors for word representation. In EMNLP, 2014.

[13] Bryan McCann, James Bradbury, Caiming Xiong, and Richard
Socher. Learned in translation: Contextualized word vectors.
In NeurIPS, 2017.

[14] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.
Deep contextualized word representations. In NAACL-HLT, 2018.

[15] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya
Sutskever. Improving language understanding by generative
pre-training. 2018. URL https://s3-us-west-2.amazonaws.
com/openai-assets/researchcovers/languageunsupervised/
languageunderstandingpaper.pdf.

[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, 2019.

[17] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE
transactions on pattern analysis and machine intelligence, 35(8):1798–1828, 2013. 

[18] Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. Character-aware neural language models. In AAAI,
2016.

[19] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas
Mikolov. Enriching word vectors with subword information. TACL, 5:135–146, 2017. doi: https://doi.org/10.1162/
tacl a 00051.

[20] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In ACL,
2016.

[21] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 1997. doi: https://doi.org/10.
1162/neco.1997.9.8.1735.

[22] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent
QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) 23
neural networks on sequence modeling. arXiv preprint
arXiv:1412.3555, 2014.

[23] Xiaodan Zhu, Parinaz Sobihani, and Hongyu Guo. Long
short-term memory over recursive structures. In International
Conference on Machine Learning, pages 1604–1612, 2015.

[24] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2017.

[25] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu.
A survey of transformers. arXiv preprint arXiv:2106.04554,
2021.

[26] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng Zhang. Star-transformer. In NAACLHLT, pages 1315–1325, 2019.

[27] Dumitru Erhan, Yoshua Bengio, Aaron C. Courville, PierreAntoine Manzagol, Pascal Vincent, and Samy Bengio. Why
does unsupervised pre-training help deep learning? J. Mach.
Learn. Res., 11:625–660, 2010. doi: https://dl.acm.org/doi/10.
5555/1756006.1756025.

[28] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing
the dimensionality of data with neural networks. Science, 313
(5786):504–507, 2006. doi: https://doi.org/10.1126/science.
1127647.

[29] GE Hinton, JL McClelland, and DE Rumelhart. Distributed
representations. In Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations,
pages 77–109. 1986.

[30] Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Chris- ´
tian Jauvin. A neural probabilistic language model. Journal of machine learning research, 3:1137–1155, 2003. doi:
https://dl.acm.org/doi/10.5555/944919.944966.

[31] Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, ´
Koray Kavukcuoglu, and Pavel P. Kuksa. Natural language
processing (almost) from scratch. J. Mach. Learn. Res., 2011.
doi: https://dl.acm.org/doi/10.5555/1953048.2078186.

[32] Quoc Le and Tomas Mikolov. Distributed representations of
sentences and documents. In ICML, pages 1188–1196, 2014.

[33] Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard
Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.
Skip-thought vectors. In NeurIPS, pages 3294–3302, 2015.

[34] Oren Melamud, Jacob Goldberger, and Ido Dagan. Context2Vec: Learning generic context embedding with bidirectional LSTM. In CoNLL, pages 51–61, 2016.

[35] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In NeurIPS, pages 3079–3087, 2015.

[36] Prajit Ramachandran, Peter J Liu, and Quoc Le. Unsupervised pretraining for sequence to sequence learning. In EMNLP,
pages 383–391, 2017.

[37] Alan Akbik, Duncan Blythe, and Roland Vollgraf. Contextual
string embeddings for sequence labeling. In COLING, pages
1638–1649, 2018.

[38] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. In ACL, pages 328–
339, 2018.

[39] Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. Cloze-driven pretraining of selfattention networks. In Kentaro Inui, Jing Jiang, Vincent Ng,and Xiaojun Wan, editors, EMNLP-IJCNLP, pages 5359– 5368, 2019.

[40] Wilson L. Taylor. “cloze procedure”: A new tool for measuring readability. Journalism Quarterly, 30(4):415–433, 1953.
doi: https://doi.org/10.1177/107769905303000401.

[41] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan
Liu. MASS: masked sequence to sequence pre-training for
language generation. In ICML, volume 97 of Proceedings of
Machine Learning Research, pages 5926–5936, 2019.

[42] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683,2019.

[43] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke
Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint
arXiv:1907.11692, 2019.

[44] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu,
Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.
Unified language model pre-training for natural language understanding and generation. In NeurIPS, pages 13042–13054,
2019.

[45] Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang,
Xiaodong Liu, Yu Wang, Songhao Piao, Jianfeng Gao, Ming
Zhou, et al. UniLMv2: Pseudo-masked language models
for unified language model pre-training. arXiv preprint
arXiv:2002.12804, 2020.

[46] Alexis Conneau and Guillaume Lample. Cross-lingual language model pretraining. In NeurIPS, pages 7057–7067, 2019.

[47] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke
Zettlemoyer, and Omer Levy. SpanBERT: Improving pretraining by representing and predicting spans. Transactions of
the Association for Computational Linguistics, 8:64–77, 2019.
doi: https://doi.org/10.1162/tacl a 00300.

[48] Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Liwei
Peng, and Luo Si. StructBERT: Incorporating language structures into pre-training for deep language understanding. In
ICLR, 2020.

[49] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,
Russ R Salakhutdinov, and Quoc V Le. XLNet: Generalized autoregressive pretraining for language understanding. In
NeurIPS, pages 5754–5764, 2019.

[50] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: denoising sequence-to-
24 QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020)
sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461,
2019.

[51] Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail
Khodak, and Hrishikesh Khandeparkar. A theoretical analysis
of contrastive unsupervised representation learning. In ICML,
pages 5628–5637, 2019.

[52] Andriy Mnih and Koray Kavukcuoglu. Learning word embeddings efficiently with noise-contrastive estimation. In NeurIPS,
pages 2265–2273, 2013.

[53] Michael Gutmann and Aapo Hyvarinen. Noise-contrastive ¨
estimation: A new estimation principle for unnormalized statistical models. In AISTATS, pages 297–304, 2010.

[54] R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon,
Karan Grewal, Philip Bachman, Adam Trischler, and Yoshua
Bengio. Learning deep representations by mutual information
estimation and maximization. In ICLR, 2019.

[55] Lingpeng Kong, Cyprien de Masson d’Autume, Lei Yu, Wang
Ling, Zihang Dai, and Dani Yogatama. A mutual information
maximization perspective of language representation learning.
In ICLR, 2019.

[56] Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA: Pre-training text encoders as
discriminators rather than generators. In ICLR, 2020.

[57] Wenhan Xiong, Jingfei Du, William Yang Wang, and Veselin
Stoyanov. Pretrained encyclopedia: Weakly supervised
knowledge-pretrained language model. In ICLR, 2020.

[58] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 2019.

[59] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. Language models are fewshot learners. In Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, 2020.

[60] Zewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-Ling
Mao, and Heyan Huang. Cross-lingual natural language generation via pre-training. In AAAI, 2019.

[61] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov,
Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer.
Multilingual denoising pre-training for neural machine translation. arXiv preprint arXiv:2001.08210, 2020.

[62] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav
Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard ´
Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.
Unsupervised cross-lingual representation learning at scale.
arXiv preprint arXiv:1911.02116, 2019.

[63] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin
Gimpel, Piyush Sharma, and Radu Soricut. ALBERT: A lite
BERT for self-supervised learning of language representations. In International Conference on Learning Representations, 2020.

[64] Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. How
to fine-tune BERT for text classification? In China National
Conference on Chinese Computational Linguistics, pages 194–
206, 2019.

[65] Zhongyang Li, Xiao Ding, and Ting Liu. Story ending prediction by transferable bert. In IJCAI, pages 1800–1806, 2019.

[66] Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, ´
Kyle Lo, Iz Beltagy, Doug Downey, and Noah A Smith. Don’t
stop pretraining: Adapt language models to domains and tasks.
In ACL, pages 8342–8360, 2020.

[67] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng
Gao. Multi-task deep neural networks for natural language
understanding. In ACL, 2019.

[68] Asa Cooper Stickland and Iain Murray. BERT and PALs:
Projected attention layers for efficient adaptation in multi-task
learning. In ICML, pages 5986–5995, 2019.

[69] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona
Attariyan, and Sylvain Gelly. Parameter-efficient transfer
learning for NLP. In ICML, pages 2790–2799, 2019.

[70] Timo Schick and Hinrich Schutze. It’s not just size that mat- ¨
ters: Small language models are also few-shot learners. In
Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June
6-11, 2021, pages 2339–2352. Association for Computational
Linguistics, 2021.

[71] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge
from language models with automatically generated prompts.
In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online,
November 16-20, 2020, pages 4222–4235. Association for
Computational Linguistics, 2020.

[72] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pretrained language models better few-shot learners. arXiv
preprint arXiv:2012.15723, 2020.

[73] Karen Hambardzumyan, Hrant Khachatrian, and Jonathan
May. WARP: word-level adversarial reprogramming. arXiv
preprint arXiv:2101.00121, 2021.

[74] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint
arXiv:2101.00190, 2021.

[75] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie
QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) 25
Qian, Zhilin Yang, and Jie Tang. GPT understands, too. arXiv
preprint arXiv:2103.10385, 2021.

[76] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong
Sun, and Qun Liu. ERNIE: enhanced language representation
with informative entities. In ACL, 2019.

[77] Matthew E. Peters, Mark Neumann, Robert L. Logan IV, Roy
Schwartz, Vidur Joshi, Sameer Singh, and Noah A. Smith.
Knowledge enhanced contextual word representations. In
EMNLP-IJCNLP, 2019.

[78] Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,
Haotang Deng, and Ping Wang. K-BERT: Enabling language
representation with knowledge graph. In AAAI, 2019.

[79] Pei Ke, Haozhe Ji, Siyang Liu, Xiaoyan Zhu, and Minlie Huang. SentiLR: Linguistic knowledge enhanced language representation for sentiment analysis. arXiv preprint arXiv:1911.02493, 2019.

[80] Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan Liu,
Juanzi Li, and Jian Tang. KEPLER: A unified model for
knowledge embedding and pre-trained language representation. arXiv preprint arXiv:1911.06136, 2019.

[81] Tianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo, Yaru
Hu, Xuanjing Huang, and Zheng Zhang. Colake: Contextualized language and knowledge embedding. In Proceedings of
the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December
8-13, 2020, pages 3660–3670. International Committee on
Computational Linguistics, 2020.

[82] Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun
Shou, Daxin Jiang, and Ming Zhou. Unicoder: A universal
language encoder by pre-training with multiple cross-lingual
tasks. In EMNLP-IJCNLP, pages 2485–2494, 2019.

[83] Julian Eisenschlos, Sebastian Ruder, Piotr Czapla, Marcin
Kadras, Sylvain Gugger, and Jeremy Howard. MultiFiT: Efficient multi-lingual language model fine-tuning. In EMNLPIJCNLP, pages 5701–5706, 2019.

[84] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen,
Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua
Wu. ERNIE: enhanced representation through knowledge
integration. arXiv preprint arXiv:1904.09223, 2019.

[85] Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang,
Shijin Wang, and Guoping Hu. Pre-training with whole word
masking for chinese BERT. arXiv preprint arXiv:1906.08101,
2019.

[86] Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang,
Yi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao Chen,
and Qun Liu. NEZHA: Neural contextualized representation for chinese language understanding. arXiv preprint
arXiv:1909.00204, 2019.

[87] Shizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, and Yonggang Wang. ZEN: pre-training chinese text encoder enhanced
by n-gram representations. arXiv preprint arXiv:1911.00720,
2019.

[88] Wietse de Vries, Andreas van Cranenburgh, Arianna Bisazza,
Tommaso Caselli, Gertjan van Noord, and Malvina Nissim. BERTje: A Dutch BERT model. arXiv preprint
arXiv:1912.09582, 2019.

[89] Louis Martin, Benjamin Muller, Pedro Javier Ortiz Su ¨ arez, ´
Yoann Dupont, Laurent Romary, Eric Villemonte de la Clerg- ´
erie, Djame Seddah, and Beno ´ ˆıt Sagot. CamemBERT: a tasty
French language model. arXiv preprint arXiv:1911.03894,
2019.

[90] Hang Le, Lo¨ıc Vial, Jibril Frej, Vincent Segonne, Maximin
Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoˆıt
Crabbe, Laurent Besacier, and Didier Schwab. FlauBERT: ´
Unsupervised language model pre-training for French. arXiv
preprint arXiv:1912.05372, 2019.

[91] Pieter Delobelle, Thomas Winters, and Bettina Berendt. RobBERT: a Dutch RoBERTa-based language model. arXiv
preprint arXiv:2001.06286, 2020.

[92] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In NeurIPS, pages 13–23,2019.

[93] Hao Tan and Mohit Bansal. LXMERT: Learning crossmodality encoder representations from transformers. In
EMNLP-IJCNLP, pages 5099–5110, 2019.

[94] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and
Kai-Wei Chang. VisualBERT: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557,
2019.

[95] Chris Alberti, Jeffrey Ling, Michael Collins, and David Reitter. Fusion of detected objects in text for visual question
answering. In EMNLP-IJCNLP, pages 2131–2140, 2019.

[96] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu
Wei, and Jifeng Dai. VL-BERT: Pre-training of generic visuallinguistic representations. In ICLR, 2020.

[97] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and
Cordelia Schmid. VideoBERT: A joint model for video and
language representation learning. In ICCV, pages 7463–7472.
IEEE, 2019.

[98] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia
Schmid. Contrastive bidirectional transformer for temporal
representation learning. arXiv preprint arXiv:1906.05743,
2019.

[99] Yung-Sung Chuang, Chi-Liang Liu, and Hung-yi Lee.
SpeechBERT: Cross-modal pre-trained language model for
end-to-end spoken question answering. arXiv preprint
arXiv:1910.11559, 2019.

[100] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim,
Sunkyu Kim, Chan Ho So, and Jaewoo Kang. BioBERT:
a pre-trained biomedical language representation model for
biomedical text mining. Bioinformatics, 2019.

[101] Iz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for scientific text. In EMNLP-IJCNLP,
26 QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020)
pages 3613–3618, 2019.

[102] Jieh-Sheng Lee and Jieh Hsiang. PatentBERT: Patent classification with fine-tuning a pre-trained BERT model. arXiv
preprint arXiv:1906.02124, 2019.

[103] Mitchell A Gordon, Kevin Duh, and Nicholas Andrews. Compressing BERT: Studying the effects of weight pruning on
transfer learning. arXiv preprint arXiv:2002.08307, 2020.

[104] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao,
Amir Gholami, Michael W Mahoney, and Kurt Keutzer. QBERT: Hessian based ultra low precision quantization of
BERT. In AAAI, 2020.

[105] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe
Wasserblat. Q8BERT: Quantized 8bit BERT. arXiv preprint
arXiv:1910.06188, 2019.

[106] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
Wolf. DistilBERT, a distilled version of BERT: smaller, faster,
cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.

[107] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen,
Linlin Li, Fang Wang, and Qun Liu. TinyBERT: Distilling
BERT for natural language understanding. arXiv preprint
arXiv:1909.10351, 2019.

[108] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang,
and Ming Zhou. MiniLM: Deep self-attention distillation for
task-agnostic compression of pre-trained transformers. arXiv
preprint arXiv:2002.10957, 2020.

[109] Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and
Ming Zhou. BERT-of-Theseus: Compressing BERT by progressive module replacing. arXiv preprint arXiv:2002.02925,
2020.

[110] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy
Lin. DeeBERT: Dynamic early exiting for accelerating BERT
inference. In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics, pages 2246–2251,
Online, July 2020. Association for Computational Linguistics.

[111] Roy Schwartz, Gabriel Stanovsky, Swabha Swayamdipta,
Jesse Dodge, and Noah A. Smith. The right tool for the
job: Matching model and instance complexities. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault,
editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July
5-10, 2020, pages 6640–6651. Association for Computational
Linguistics, 2020.

[112] Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Haotang
Deng, and Qi Ju. FastBERT: a self-distilling BERT with
adaptive inference time. In ACL, 2020.

[113] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley,
Ke Xu, and Furu Wei. Bert loses patience: Fast and robust
inference with early exit. arXiv preprint arXiv:2006.04152,
2020.

[114] Kaiyuan Liao, Yi Zhang, Xuancheng Ren, Qi Su, Xu Sun, and
Bin He. A global past-future early exit method for accelerating inference of pre-trained language models. In Proceedings
of the 2021 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2021, Online, June 6-11, 2021,
pages 2013–2023. Association for Computational Linguistics,
2021.

[115] Tianxiang Sun, Yunhua Zhou, Xiangyang Liu, Xinyu Zhang,
Hao Jiang, Zhao Cao, Xuanjing Huang, and Xipeng Qiu.
Early exiting with ensemble internal classifiers. arXiv preprint
arXiv: 2105.13792, 2021.

[116] Xiaonan Li, Yunfan Shao, Tianxiang Sun, Hang Yan, Xipeng
Qiu, and Xuanjing Huang. Accelerating bert inference for
sequence labeling via early-exit. In ACL, 2021.

[117] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. In
HLT-NAACL, pages 746–751, 2013.

[118] Dana Rubinstein, Effi Levi, Roy Schwartz, and Ari Rappoport.
How well do distributional models capture different types of
semantic knowledge? In ACL, pages 726–730, 2015.

[119] Abhijeet Gupta, Gemma Boleda, Marco Baroni, and Sebastian
Pado. Distributional vectors encode referential attributes. In ´
EMNLP, pages 12–21, 2015.

[120] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme,
Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick. What
do you learn from context? probing for sentence structure in
contextualized word representations. In ICLR, 2019.

[121] Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E.
Peters, and Noah A. Smith. Linguistic knowledge and transferability of contextual representations. In NAACL-HLT, pages
1073–1094, 2019.

[122] Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In Anna Korhonen, David R.
Traum, and Llu´ıs Marquez, editors, ` ACL, pages 4593–4601,
2019.

[123] Yoav Goldberg. Assessing BERT’s syntactic abilities. arXiv
preprint arXiv:1901.05287, 2019.

[124] Allyson Ettinger. What BERT is not: Lessons from a new suite
of psycholinguistic diagnostics for language models. TACL,
8:34–48, 2020.

[125] John Hewitt and Christopher D. Manning. A structural probe
for finding syntax in word representations. In NAACL-HLT,
pages 4129–4138, 2019.

[126] Ganesh Jawahar, Benoˆıt Sagot, and Djame Seddah. What does ´
BERT learn about the structure of language? In ACL, pages
3651–3657, 2019.

[127] Taeuk Kim, Jihun Choi, Daniel Edmiston, and Sang goo Lee.
Are pre-trained language models aware of phrases? simple
but strong baselines for grammar induction. In ICLR, 2020.

[128] Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda B Viegas, Andy Coenen, Adam Pearce, and Been Kim. Visualizing
and measuring the geometry of BERT. In NeurIPS, pages
8592–8600, 2019.
QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) 27

[129] Fabio Petroni, Tim Rocktaschel, Sebastian Riedel, Patrick ¨
S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H.
Miller. Language models as knowledge bases? In EMNLPIJCNLP, pages 2463–2473, 2019.

[130] Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. How can we know what language models know? arXiv
preprint arXiv:1911.12543, 2019.

[131] Nina Porner, Ulli Waltinger, and Hinrich Sch ¨ utze. BERT is not ¨
a knowledge base (yet): Factual knowledge vs. name-based
reasoning in unsupervised QA. CoRR, abs/1911.03681, 2019.

[132] Nora Kassner and Hinrich Schutze. Negated LAMA: birds ¨
cannot fly. arXiv preprint arXiv:1911.03343, 2019.

[133] Zied Bouraoui, Jose Camacho-Collados, and Steven Schock- ´
aert. Inducing relational knowledge from BERT. In AAAI,
2019.

[134] Joe Davison, Joshua Feldman, and Alexander M. Rush. Commonsense knowledge mining from pretrained models. In
EMNLP-IJCNLP, pages 1173–1178, 2019.

[135] Anne Lauscher, Ivan Vulic, Edoardo Maria Ponti, Anna Korhonen, and Goran Glavas. Informing unsupervised pretraining with external linguistic knowledge. arXiv preprint
arXiv:1909.02339, 2019.

[136] Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing
Huang, Jianshu Ji, Guihong Cao, Daxin Jiang, and Ming Zhou.
K-adapter: Infusing knowledge into pre-trained models with
adapters. arXiv preprint arXiv:2002.01808, 2020.

[137] Yoav Levine, Barak Lenz, Or Dagan, Dan Padnos, Or Sharir,
Shai Shalev-Shwartz, Amnon Shashua, and Yoav Shoham.
SenseBERT: Driving some sense into BERT. arXiv preprint
arXiv:1908.05646, 2019.

[138] Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and Minlie
Huang. A knowledge-enhanced pretraining model for commonsense story generation. arXiv preprint arXiv:2001.05139,
2020.

[139] Bin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu,
Nicholas Jing Yuan, and Tong Xu. Integrating graph contextualized knowledge into pre-trained language models. arXiv
preprint arXiv:1912.00147, 2019.

[140] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen.
Knowledge graph and text jointly embedding. In EMNLP,
pages 1591–1601, 2014.

[141] Huaping Zhong, Jianwen Zhang, Zhen Wang, Hai Wan, and
Zheng Chen. Aligning knowledge and text embeddings by
entity descriptions. In EMNLP, pages 267–272, 2015.

[142] Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and
Maosong Sun. Representation learning of knowledge graphs
with entity descriptions. In IJCAI, 2016.

[143] Jiacheng Xu, Xipeng Qiu, Kan Chen, and Xuanjing Huang.
Knowledge graph representation with jointly structural and
textual encoding. In IJCAI, pages 1318–1324, 2017.

[144] An Yang, Quan Wang, Jing Liu, Kai Liu, Yajuan Lyu, Hua
Wu, Qiaoqiao She, and Sujian Li. Enhancing pre-trained
language representations with rich knowledge for machine
reading comprehension. In ACL, pages 2346–2357, 2019.

[145] Robert L. Logan IV, Nelson F. Liu, Matthew E. Peters, Matt
Gardner, and Sameer Singh. Barack’s wife hillary: Using
knowledge graphs for fact-aware language modeling. In ACL,
2019.

[146] Hiroaki Hayashi, Zecong Hu, Chenyan Xiong, and Graham
Neubig. Latent relation language models. In AAAI, 2019.

[147] Manaal Faruqui and Chris Dyer. Improving vector space word
representations using multilingual correlation. In EACL, pages
462–471, 2014.

[148] Minh-Thang Luong, Hieu Pham, and Christopher D Manning.
Bilingual word representations with monolingual quality in
mind. In Proceedings of the 1st Workshop on Vector Space
Modeling for Natural Language Processing, pages 151–159,
2015.

[149] Karan Singla, Dogan Can, and Shrikanth Narayanan. A multi- ˘
task approach to learning multilingual representations. In
ACL, pages 214–220, 2018.

[150] Telmo Pires, Eva Schlinger, and Dan Garrette. How multilingual is multilingual BERT? In ACL, pages 4996–5001,
2019.

[151] Karthikeyan K, Zihan Wang, Stephen Mayhew, and Dan Roth.
Cross-lingual ability of multilingual BERT: An empirical
study. In ICLR, 2020.

[152] Antti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma, Juhani
Luotolahti, Tapio Salakoski, Filip Ginter, and Sampo Pyysalo.
Multilingual is not enough: BERT for Finnish. arXiv preprint
arXiv:1912.07076, 2019.

[153] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian,
Hua Wu, and Haifeng Wang. ERNIE 2.0: A continual pretraining framework for language understanding. In AAAI,
2019.

[154] Yuri Kuratov and Mikhail Arkhipov. Adaptation of deep bidirectional multilingual transformers for russian language. arXiv
preprint arXiv:1905.07213, 2019.

[155] Wissam Antoun, Fady Baly, and Hazem Hajj. AraBERT:
Transformer-based model for Arabic language understanding.
arXiv preprint arXiv:2003.00104, 2020.

[156] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan,
Tianrui Li, Xilin Chen, and Ming Zhou. UniViLM: A unified
video and language pre-training model for multimodal understanding and generation. arXiv preprint arXiv:2002.06353,
2020.

[157] Gen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and Ming Zhou.
Unicoder-vl: A universal encoder for vision and language by
cross-modal pre-training. In AAAI, 2020.

[158] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,
Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.
UNITER: learning universal image-text representations. arXiv
preprint arXiv:1909.11740, 2019.

[159] Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clin-
28 QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020)
icalBERT: Modeling clinical notes and predicting hospital
readmission. arXiv preprint arXiv:1904.05342, 2019.

[160] Emily Alsentzer, John R. Murphy, Willie Boag, Wei-Hung
Weng, Di Jin, Tristan Naumann, and Matthew B. A. McDermott. Publicly available clinical BERT embeddings. arXiv
preprint arXiv:1904.03323, 2019.

[161] Zongcheng Ji, Qiang Wei, and Hua Xu. BERT-based ranking for biomedical entity normalization. arXiv preprint
arXiv:1908.03548, 2019.

[162] Matthew Tang, Priyanka Gandhi, Md Ahsanul Kabir, Christopher Zou, Jordyn Blakey, and Xiao Luo. Progress notes classification and keyword extraction using attention-based deep
learning models with BERT. arXiv preprint arXiv:1910.05786,
2019.

[163] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J
Liu. PEGASUS: Pre-training with extracted gap-sentences for
abstractive summarization. arXiv preprint arXiv:1912.08777,
2019.

[164] Shaolei Wang, Wanxiang Che, Qi Liu, Pengda Qin, Ting Liu,
and William Yang Wang. Multi-task self-supervised learning
for disfluency detection. In AAAI, 2019.

[165] Cristian Bucilua, Rich Caruana, and Alexandru NiculescuMizil. Model compression. In KDD, pages 535–541, 2006.

[166] Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan,
Yin Yang, Deming Chen, Marianne Winslett, Hassan Sajjad,
and Preslav Nakov. Compressing large-scale transformerbased models: A case study on BERT. arXiv preprint
arXiv:2002.11985, 2020.

[167] Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Hawq: Hessian aware quantization
of neural networks with mixed-precision. In ICCV, pages
293–302, 2019.

[168] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint
arXiv:1503.02531, 2015.

[169] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for BERT model compression. In EMNLPIJCNLP, pages 4323–4332, 2019.

[170] Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Well-read students learn better: The impact of
student initialization on knowledge distillation. arXiv preprint
arXiv:1908.08962, 2019.

[171] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. MobileBERT: a compact taskagnostic BERT for resource-limited devices. arXiv preprint
arXiv:2004.02984, 2020.

[172] Sanqiang Zhao, Raghav Gupta, Yang Song, and Denny Zhou.
Extreme language model compression with optimal subwords
and shared projections. arXiv preprint arXiv:1909.11687,
2019.

[173] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer
in BERTology: What we know about how BERT works. arXiv
preprint arXiv:2002.12327, 2020.

[174] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen
heads really better than one? In NeurIPS, pages 14014–14024,
2019.

[175] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and
Ivan Titov. Analyzing multi-head self-attention: Specialized
heads do the heavy lifting, the rest can be pruned. In ACL,
pages 5797–5808, 2019.

[176] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob
Uszkoreit, and Lukasz Kaiser. Universal transformers. In
ICLR, 2019.

[177] Wenhao Lu, Jian Jiao, and Ruofei Zhang. TwinBERT: Distilling knowledge to twin-structured BERT models for efficient
retrieval. arXiv preprint arXiv:2002.06275, 2020.

[178] Henry Tsai, Jason Riesa, Melvin Johnson, Naveen Arivazhagan, Xin Li, and Amelia Archer. Small and practical BERT
models for sequence labeling. In EMNLP-IJCNLP, pages
3632–3636, 2019.

[179] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng
Gao. Improving multi-task deep neural networks via knowledge distillation for natural language understanding. arXiv
preprint arXiv:1904.09482, 2019.

[180] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-specific knowledge
from BERT into simple neural networks. arXiv preprint
arXiv:1903.12136, 2019.

[181] Yew Ken Chia, Sam Witteveen, and Martin Andrews. Transformer to CNN: Label-scarce distillation for efficient text
classification. arXiv preprint arXiv:1909.03508, 2019.

[182] Surat Teerapittayanon, Bradley McDanel, and H. T. Kung.
Branchynet: Fast inference via early exiting from deep neural
networks. In 23rd International Conference on Pattern Recognition, ICPR 2016, Canc´un, Mexico, December 4-8, 2016,
pages 2464–2469. IEEE, 2016.

[183] Yigitcan Kaya, Sanghyun Hong, and Tudor Dumitras.
Shallow-deep networks: Understanding and mitigating network overthinking. In Proceedings of the 36th International
Conference on Machine Learning, ICML 2019, 9-15 June
2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 3301–3310. PMLR,
2019.

[184] Alex Graves. Adaptive computation time for recurrent neural
networks. arXiv preprint arXiv:1603.08983, 2016.

[185] Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli.
Depth-adaptive transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020. OpenReview.net, 2020.

[186] Yijin Liu, Fandong Meng, Jie Zhou, Yufeng Chen, and Jinan
Xu. Faster depth-adaptive transformers. In Thirty-Fifth AAAI
Conference on Artificial Intelligence, AAAI 2021, Thirty-Third
Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational
QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) 29
Advances in Artificial Intelligence, EAAI 2021, Virtual Event,
February 2-9, 2021, pages 13424–13432. AAAI Press, 2021.

[187] Keli Xie, Siyuan Lu, Meiqi Wang, and Zhongfeng Wang. Elbert: Fast albert with confidence-window based early exit. In
ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7713–
7717. IEEE, 2021.

[188] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering,
22(10):1345–1359, 2009. doi: https://doi.org/10.1109/TKDE.
2009.191.

[189] Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad,
and James Glass. What do neural machine translation models
learn about morphology? In ACL, pages 861–872, 2017.

[190] Matthew E. Peters, Sebastian Ruder, and Noah A. Smith. To
tune or not to tune? adapting pretrained representations to
diverse tasks. In Proceedings of the 4th Workshop on Representation Learning for NLP, RepL4NLP@ACL 2019, Florence,
Italy, August 2, 2019, pages 7–14, 2019.

[191] Ming Zhong, Pengfei Liu, Danqing Wang, Xipeng Qiu, and
Xuanjing Huang. Searching for effective neural extractive
summarization: What works and what’s next. In ACL, pages
1049–1058, 2019.

[192] Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang
Zhou, Houqiang Li, and Tieyan Liu. Incorporating BERT into
neural machine translation. In ICLR, 2020.

[193] Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi,
Hannaneh Hajishirzi, and Noah Smith. Fine-tuning pretrained
language models: Weight initializations, data orders, and early
stopping. arXiv preprint arXiv:2002.06305, 2020.

[194] Jason Phang, Thibault Fevry, and Samuel R Bowman. Sen- ´
tence encoders on STILTs: Supplementary training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088,
2018.

[195] Siddhant Garg, Thuy Vu, and Alessandro Moschitti. Tanda:
Transfer and adapt pre-trained transformer models for answer
sentence selection. In AAAI, 2019.

[196] Yige Xu, Xipeng Qiu, Ligao Zhou, and Xuanjing Huang.
Improving BERT fine-tuning via self-ensemble and selfdistillation. arXiv preprint arXiv:2002.10345, 2020.

[197] Alexandra Chronopoulou, Christos Baziotis, and Alexandros
Potamianos. An embarrassingly simple approach for transfer
learning from pretrained language models. In NAACL-HLT,
pages 2089–2095, 2019.

[198] Xiang Lisa Li and Jason Eisner. Specializing word embeddings (for parsing) by information bottleneck. In EMNLPIJCNLP, pages 2744–2754, 2019.

[199] Teven Le Scao and Alexander M. Rush. How many data
points is a prompt worth? In Proceedings of the 2021 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies,
NAACL-HLT 2021, Online, June 6-11, 2021, pages 2627–
2636. Association for Computational Linguistics, 2021.

[200] Timo Schick and Hinrich Schutze. Exploiting cloze-questions ¨
for few-shot text classification and natural language inference.
In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main
Volume, EACL 2021, Online, April 19 - 23, 2021, pages 255–
269. Association for Computational Linguistics, 2021.

[201] Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig.
How can we know what language models know. Trans. Assoc.
Comput. Linguistics, 8:423–438, 2020.

[202] Chi Sun, Luyao Huang, and Xipeng Qiu. Utilizing BERT
for aspect-based sentiment analysis via constructing auxiliary
sentence. In NAACL-HLT, 2019.

[203] Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. In Proceedings of
the 2021 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2021, Online, June 6-11, 2021,
pages 5203–5212. Association for Computational Linguistics,
2021.

[204] Zexuan Zhong, Dan Friedman, and Danqi Chen. Factual probing is [MASK]: learning vs. learning to recall. In Proceedings
of the 2021 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2021, Online, June 6-11, 2021,
pages 5017–5033. Association for Computational Linguistics,
2021.

[205] Brian Lester, Rami Al-Rfou, and Noah Constant. The power
of scale for parameter-efficient prompt tuning. arXiv preprint
arXiv:2104.08691, 2021.

[206] Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord,
Pradeep Dasigi, Nelson F. Liu, Matthew Peters, Michael
Schmitz, and Luke S. Zettlemoyer. Allennlp: A deep semantic
natural language processing platform. 2017.

[207] Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. CTRL: A conditional transformer language model for controllable generation. arXiv
preprint arXiv:1909.05858, 2019.

[208] Jesse Vig. A multiscale visualization of attention in the transformer model. In ACL, 2019.

[209] Benjamin Hoover, Hendrik Strobelt, and Sebastian Gehrmann.
exbert: A visual analysis tool to explore learned representations in transformers models. arXiv preprint
arXiv:1910.05276, 2019.

[210] Ziqing Yang, Yiming Cui, Zhipeng Chen, Wanxiang Che, Ting
Liu, Shijin Wang, and Guoping Hu. Textbrewer: An opensource knowledge distillation toolkit for natural language processing. arXiv preprint arXiv:2002.12620, 2020.

[211] Yuxuan Wang, Yutai Hou, Wanxiang Che, and Ting Liu. From
static to dynamic word representations: a survey. International
Journal of Machine Learning and Cybernetics, pages 1–20,
2020. doi: https://doi.org/10.1007/s13042-020-01069-8.
30 QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020)

[212] Qi Liu, Matt J Kusner, and Phil Blunsom. A survey on contextual embeddings. arXiv preprint arXiv:2003.07278, 2020.

[213] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
Omer Levy, and Samuel R. Bowman. GLUE: A multi-task
benchmark and analysis platform for natural language understanding. In ICLR, 2019.

[214] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet
Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R.
Bowman. SuperGLUE: A stickier benchmark for generalpurpose language understanding systems. In NeurIPS, pages
3261–3275, 2019.

[215] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy
Liang. Squad: 100, 000+ questions for machine comprehension of text. In Jian Su, Xavier Carreras, and Kevin Duh,
editors, EMNLP, pages 2383–2392, 2016.

[216] Siva Reddy, Danqi Chen, and Christopher D. Manning. CoQA:
A conversational question answering challenge. TACL, 7:249–
266, 2019. doi: https://doi.org/10.1162/tacl a 00266.

[217] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,
William W. Cohen, Ruslan Salakhutdinov, and Christopher D.
Manning. HotpotQA: A dataset for diverse, explainable multihop question answering. In EMNLP, pages 2369–2380, 2018.

[218] Zhuosheng Zhang, Junjie Yang, and Hai Zhao. Retrospective
reader for machine reading comprehension. arXiv preprint
arXiv:2001.09694, 2020.

[219] Ying Ju, Fubang Zhao, Shijie Chen, Bowen Zheng, Xuefeng
Yang, and Yunfeng Liu. Technical report on conversational
question answering. arXiv preprint arXiv:1909.10772, 2019.

[220] Ming Tu, Kevin Huang, Guangtao Wang, Jing Huang, Xiaodong He, and Bowen Zhou. Select, answer and explain:
Interpretable multi-hop reading comprehension over multiple
documents. In AAAI, 2020.

[221] Enkhbold Bataa and Joshua Wu. An investigation of transfer
learning-based sentiment analysis in japanese. In ACL, 2019.

[222] Hu Xu, Bing Liu, Lei Shu, and Philip S. Yu. BERT posttraining for review reading comprehension and aspect-based
sentiment analysis. In NAACL-HLT, 2019.

[223] Alexander Rietzler, Sebastian Stabinger, Paul Opitz, and Stefan Engl. Adapt or get left behind: Domain adaptation through
BERT language model finetuning for aspect-target sentiment
classification. arXiv preprint arXiv:1908.11860, 2019.

[224] Akbar Karimi, Leonardo Rossi, Andrea Prati, and Katharina
Full. Adversarial training for aspect-based sentiment analysis
with BERT. arXiv preprint arXiv:2001.11316, 2020.

[225] Youwei Song, Jiahai Wang, Zhiwei Liang, Zhiyue Liu, and
Tao Jiang. Utilizing BERT intermediate layers for aspect based
sentiment analysis and natural language inference. arXiv
preprint arXiv:2002.04815, 2020.

[226] Xin Li, Lidong Bing, Wenxuan Zhang, and Wai Lam. Exploiting BERT for end-to-end aspect-based sentiment analysis. In
W-NUT@EMNLP, 2019.

[227] Xing Wu, Tao Zhang, Liangjun Zang, Jizhong Han, and
Songlin Hu. ”mask and infill” : Applying masked language
model to sentiment transfer. In IJCAI, 2019.

[228] Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula,
and Russell Power. Semi-supervised sequence tagging with
bidirectional language models. In ACL, pages 1756–1765,
2017.

[229] Liyuan Liu, Xiang Ren, Jingbo Shang, Xiaotao Gu, Jian Peng,
and Jiawei Han. Efficient contextualized representation: Language model pruning for sequence labeling. In EMNLP, pages
1215–1225, 2018.

[230] Kai Hakala and Sampo Pyysalo. Biomedical named entity
recognition with multilingual BERT. In BioNLP Open Shared
Tasks@EMNLP, pages 56–61, 2019.

[231] Sergey Edunov, Alexei Baevski, and Michael Auli. Pre-trained
language model representations for language generation. In
Jill Burstein, Christy Doran, and Thamar Solorio, editors,
NAACL-HLT, pages 4052–4059, 2019.

[232] Stephane Clinchant, Kweon Woo Jung, and Vassilina
Nikoulina. On the use of BERT for neural machine translation.
In Proceedings of the 3rd Workshop on Neural Generation
and Translation, Hong Kong, 2019.

[233] Kenji Imamura and Eiichiro Sumita. Recycling a pre-trained
BERT encoder for neural machine translation. In Proceedings
of the 3rd Workshop on Neural Generation and Translation,
Hong Kong, November 2019.

[234] Xingxing Zhang, Furu Wei, and Ming Zhou. HIBERT: Document level pre-training of hierarchical bidirectional transformers for document summarization. In ACL, pages 5059–5069, 2019.

[235] Yang Liu and Mirella Lapata. Text summarization with pretrained encoders. In EMNLP/IJCNLP, 2019.

[236] Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Xipeng Qiu, and Xuan-Jing Huang. Extractive summarization as text matching. In ACL, 2020.

[237] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is BERT really robust? natural language attack on text classification and entailment. In AAAI, 2019.

[238] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and
Sameer Singh. Universal adversarial triggers for attacking and
analyzing NLP. In EMNLP-IJCNLP, pages 2153–2162, 2019.

[239] Lichao Sun, Kazuma Hashimoto, Wenpeng Yin, Akari Asai, Jia Li, Philip Yu, and Caiming Xiong. Adv-BERT: BERT
is not robust on misspellings! generating nature adversarial samples on BERT. arXiv preprint arXiv:2003.04985, 2020.

[240] Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu. BERT-ATTACK: Adversarial attack against
BERT using BERT. arXiv preprint arXiv:2004.09984, 2020.

[241] Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. FreeLB: Enhanced adversarial training for
natural language understanding. In ICLR, 2020.

[242] Xiulei Liu, Hao Cheng, Peng cheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao. Adversarial QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) 31 training for large neural language models. arXiv preprint arXiv:2004.08994, 2020.

[243] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. MegatronLM: Training multi-billion parameter language models using gpu model parallelism. arXiv preprint arXiv:1909.08053, 2019.

[244] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell,
Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In ACL, pages 2978–2988, 2019.

[245] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In ICLR, 2017.

[246] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration for deep neural networks.
arXiv preprint arXiv:1710.09282, 2017.

[247] Xing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han, and Songlin Hu. Conditional BERT contextual augmentation. In
International Conference on Computational Science, pages 84–95, 2019.

[248] Varun Kumar, Ashutosh Choudhary, and Eunah Cho. Data augmentation using pre-trained transformer models. arXiv
preprint arXiv:2003.02245, 2020.

[249] Alejandro Barredo Arrieta, Natalia D´ıaz-Rodr´ıguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado,
Salvador Garc´ıa, Sergio Gil-Lopez, Daniel Molina, Richard ´ Benjamins, et al. Explainable artificial intelligence (xai):
Concepts, taxonomies, opportunities and challenges toward responsible ai. Information Fusion, 58:82–115, 2020.

[250] Sarthak Jain and Byron C Wallace. Attention is not explanation. In NAACL-HLT, pages 3543–3556, 2019.

[251] Sofia Serrano and Noah A Smith. Is attention interpretable?In ACL, pages 2931–2951, 2019.
