# Natural Language Processing
- 🤗[papers-with- code](https://paperswithcode.com/area/natural-language-processing)1769 benchmarks • 541 tasks • 1515 datasets • 15957 papers with code


## [Language Modelling](https://paperswithcode.com/task/language-modelling)
- 1989 papers with code • 37 benchmarks • 130 datasets
- 🤗[huggingface/transformers](https://github.com/huggingface/transformers)

- NLP最新發展:Large-scale pretrained language models
- Amazon AlexaTM 20B(202208)
  - [新聞:Amazon百億參數模型小樣本學習勝過GPT-3、PaLM千億參數大模型(2022-08-04)](https://www.ithome.com.tw/news/152283)
  - [論文:AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model(202208)](https://arxiv.org/abs/2208.01448)
  - [20B-parameter Alexa model sets new marks in few-shot learning(20220802)](https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning)
- Google PaLM(202204)
  - [論文:PaLM: Scaling Language Modeling with Pathways(Google Research)(202204)](https://arxiv.org/abs/2204.02311)
  - [實作:PaLM - Pytorch](https://github.com/lucidrains/PaLM-pytorch)
- OpenAI GPT-3(2020)
  - [Language Models are Few-Shot Learners(202005)](https://arxiv.org/pdf/2005.14165.pdf)
  - [微軟於Microsoft Power Apps中嵌入GPT-3，以自然語言就能開發應用(2021-05-27)](https://www.ithome.com.tw/news/144649)
  - 👍🏻👍🏻[gpt3-and-cybersecurity/spam_detector](https://github.com/sophos/gpt3-and-cybersecurity/tree/main/spam_detector) 
  - [論文:Improving Short Text Classification With Augmented Data Using GPT-3(202205)](https://arxiv.org/abs/2205.10981)
  - [論文:Language Models are Few-Shot Learners(202005)](https://arxiv.org/abs/2005.14165)
- Google Text-To-Text Transfer Transformer(T5)
  - [論文:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer(201910)](https://arxiv.org/abs/1910.10683)
  - [T5X:GITHUB](https://github.com/google-research/text-to-text-transfer-transformer)
- HyperCLOVA(2021)
  - [AI趨勢周報第170期：Naver發布GPT-3等級的超大韓文模型HyperCLOVA，懂語言還要懂圖片和影片(2021-06-05)]()
  - HyperCLOVA有2,040億參數，比公認是NLP世界標準的GPT-3，還要多出1,750億個參數。參數越多，就越能細緻地辨識語言。 
- XLNet(2019)
  - [XLNet: Generalized Autoregressive Pretraining for Language Understanding(201906)](https://arxiv.org/abs/1906.08237)
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach(201907)](https://arxiv.org/abs/1907.11692)
- [Cross-lingual Language Model Pretraining(201901)](https://arxiv.org/abs/1901.07291)
- Transformer-XL(2019)
  - [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context(201901)](https://arxiv.org/abs/1901.02860)
- BERT(2018)
  - [論文:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(201810)](https://arxiv.org/abs/1810.04805)
  - [BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270) 
  - YOUTUE影片
    - [BERT Neural Network - EXPLAINED!](https://www.youtube.com/watch?v=xI0HHN5XKDo)
  - Pytorch實作
    - [PYTORCH-TRANSFORMERS By HuggingFace Teams你要的全在此](https://pytorch.org/hub/huggingface_pytorch-transformers/)
    - []()
      - pip install bert-pytorch 
- ULM-Fit(201801)Universal Language Model Fine-tuning 
  - [Universal Language Model Fine-tuning for Text Classification(201801)](https://arxiv.org/abs/1801.06146) 
  - Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. 
  -  propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. 
  -  Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. 
  -  Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. 
  -  We open-source our pretrained models and code.
- ELMo(201802)
  - [Deep contextualized word representations(201802)](https://arxiv.org/pdf/1802.05365.pdf)  
- GPT-2(2019):
  - [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) 
  - [Pytorch版本](https://github.com/huggingface/transformers)
  - [GPT-2中文版本](https://github.com/Morizeyao/GPT2-Chinese)
    - [NLP模型應用之三：GPT與GPT-2](https://www.twblogs.net/a/5efb005bfa148015395f47a8) 
    - git clone https://github.com/Morizeyao/GPT2-Chinese 
- OpenAI GPT:Generative Pre-Training(201808)
  - [Improving Language Understandingby Generative Pre-Training(2018)](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) 

## [Text Classification](https://paperswithcode.com/task/text-classification)
- 743 papers with code • 107 benchmarks • 102 datasets
- [REVIEW: Text Classification Algorithms: A Survey(201904)](https://arxiv.org/pdf/1904.08067v4.pdf)
## Document Classification
- [DocBERT: BERT for Document Classification(201904)](https://arxiv.org/abs/1904.08398)
  - 👍🏻👍[PyTorch實作](https://github.com/castorini/hedwig) 
- [Document Classification using BERT](https://www.kaggle.com/code/merishnasuwal/document-classification-using-bert)
