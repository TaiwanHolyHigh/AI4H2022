# week6_AI_2_NLP_RNN

# NLP(Natural language processing) 自然語言處理 
- [史丹佛大學 課程CS224n: Natural Language Processing with Deep Learning](https://web.stanford.edu/class/cs224n/)
- NLTK 工具集 |LTP 工具集

## RNN
- RNN Model
  - RNN
  - LSTM
  - bidirectional LSTM
  - GRU
- Text classification(文章分類)
  - 資料集
    IMDB large movie review dataset
  - 各種分析技術
    - 【TensorFlow 官方教學課程】[Text classification with an RNN](https://www.tensorflow.org/text/tutorials/text_classification_rnn)
    - 【TensorFlow 官方教學課程】 [Classify text with BERT](https://www.tensorflow.org/text/tutorials/classify_text_with_bert)
- Time Series(時間序列分析)
  - [Modern Time Series Forecasting with Python(2022)](https://www.packtpub.com/product/modern-time-series-forecasting-with-python/9781803246802) [GITHUB](https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python)
## Language Model
- 靜態詞向量預訓練模型
  - 神經網路語言模型
  - Word2vec 詞向量
    - 經典論文[Show, Attend and Tell: Neural Image Caption Generation with Visual Attention|Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio](https://arxiv.org/abs/1502.03044)
    - 【TensorFlow 官方教學課程】[Word embeddings](https://www.tensorflow.org/text/guide/word_embeddings)
  - GloVe 詞向量word2vec

- sequence-to-sequence (seq2seq) model

## pretrained Model
- REVIEW[Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey(2023)](https://arxiv.org/abs/2302.10035) [GITHUB](https://github.com/wangxiao5791509/MultiModal_BigModels_Survey)
- [On the Opportunities and Risks of Foundation Models(2021)](https://arxiv.org/abs/2108.07258)
- [A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT(2023)](https://arxiv.org/abs/2302.09419)
- BOOKS [全中文自然語言處理：Pre-Trained Model 方法最新實戰|車萬翔、郭江、崔一鳴 著(2022)](https://www.tenlong.com.tw/products/9789860776942?list_name=srh)
- Transformer 2017
  - 經典論文[Attention Is All You Need](https://arxiv.org/abs/1706.03762)
  - BOOKS [Transformers for Natural Language Processing - Second Edition(2022)](https://www.packtpub.com/product/transformers-for-natural-language-processing-second-edition/9781803247335) [GITHUB](https://github.com/Denis2054/Transformers-for-NLP-2nd-Edition)
  - BOOKS [Mastering Transformers(2021)](https://www.packtpub.com/product/mastering-transformers/9781801077651) [GITHUB](https://github.com/PacktPublishing/Mastering-Transformers)
  - huggingface[Transformers|State-of-the-art Machine Learning for PyTorch, TensorFlow, and JAX](https://huggingface.co/docs/transformers/index)
- BERT(Bidirectional Encoder Representations from Transformers) 2018
  - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova](https://arxiv.org/abs/1810.04805)
  - 參考書籍
    - [Getting Started with Google BERT(2021)](https://www.packtpub.com/product/getting-started-with-google-bert/9781838821593)  [GITHUB](https://github.com/PacktPublishing/Getting-Started-with-Google-BERT)
    - [基於 BERT 模型的自然語言處理實戰|李金洪](https://www.tenlong.com.tw/products/9787121414084?list_name=sp)
  - BERT Variants: ALBERT, RoBERTa, ELECTRA, and SpanBERT ........
- XLNet 2019
  - [XLNet: Generalized Autoregressive Pretraining for Language Understanding(2019)](https://arxiv.org/abs/1906.08237) [GITHUB](https://github.com/zihangdai/xlnet)
  - [XLNet: Generalized Autoregressive Pretraining for Language Understanding | AISC](https://www.youtube.com/watch?v=Mgck4XFR9GA)
- DeBERTa 2019
- GPT-3(Generative Pre-trained Transformer 3)  2020
  - [Exploring GPT-3](https://www.packtpub.com/product/exploring-gpt-3/9781800563193)  
- Swin Transformer 2021
  - [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(2021)](https://arxiv.org/abs/2103.14030)
  - [Swin Transformer V2: Scaling Up Capacity and Resolution(2021)](https://arxiv.org/abs/2111.09883)
- MAE
- DALLE-E
- ChatGPT 2022
- Google Bard 2023.2
- 攻擊pretrained Model
  - [TrojText: Test-time Invisible Textual Trojan Insertion(2023)](https://arxiv.org/abs/2303.02242)
  
